{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2baedad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "\n",
    "path_str = \"\"\n",
    "tmp_df = pd.DataFrame()\n",
    "\n",
    "def merge_area_data():\n",
    "    # 행정동별 데이터 로드\n",
    "    df = pd.read_csv(path_str + \"행정동_컬럼추가_최종ver.csv\", index_col=0)\n",
    "    df.rename(columns={\"인구수\": \"MZ_POP_CNT\"}, inplace=True)\n",
    "\n",
    "    # 인구 밀도 데이터 로드\n",
    "    density_df = pd.read_excel(path_str + '인구밀도.xlsx')\n",
    "    density_df['GU_DONG'] = density_df['GU'] + density_df['DONG']\n",
    "    df['GU_DONG'] = df['GU'] + df['DONG']\n",
    "    density_df.drop(['GU', 'DONG', 'POP', 'DENSITY'], axis=1, inplace=True)\n",
    "\n",
    "    # 행정동 데이터, 밀도 데이터 병합\n",
    "    tmp = pd.merge(df, density_df, on='GU_DONG')\n",
    "    tmp.drop(['GU_DONG'], axis=1, inplace=True)\n",
    "\n",
    "    # 컬럼 순서 정렬\n",
    "    tmp = tmp[['GU', 'DONG', 'DONG_CODE', 'AREA', 'ACADEMY_NUM', 'KINDER_NUM', 'FIRE_NUM',\n",
    "               'ELE_SCH_NUM', 'MID_SCH_NUM', 'HIGH_SCH_NUM', 'CCTV_NUM', 'POLICE_NUM',\n",
    "               'BIKE_NUM', 'CAR_SHR_NUM', 'SUBWAY_NUM', 'SAFE_DLVR_NUM', 'DPTM_NUM',\n",
    "               'ANI_HSPT_NUM', 'PHARM_NUM', 'LEISURE_NUM', 'KIDS_NUM', 'SPORT_NUM',\n",
    "               'GYM_NUM', 'GOLF_NUM', 'STARBUCKS_NUM', 'MC_NUM', 'CON_NUM',\n",
    "               'NOISE_VIBRATION_NUM', 'CHILD_MED_NUM', 'CAFE_NUM', 'PARK_NUM',\n",
    "               'HOSPITAL_NUM', 'BUS_CNT', 'RETAIL_NUM', 'COLIVING_NUM', 'MZ_POP_CNT', 'VEGAN_CNT']]\n",
    "    # 불필요 컬럼 제거\n",
    "    tmp = tmp.drop(['SPORT_NUM'], axis=1)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def assembling_features(df):\n",
    "    global tmp_df\n",
    "    # 피쳐합\n",
    "    tmp_df = df.copy()\n",
    "    # 교통\n",
    "    tmp_df['교통'] = tmp_df['SUBWAY_NUM'] + 0.93 * tmp_df['BUS_CNT'] + 0.06 * tmp_df['BIKE_NUM']\n",
    "    tmp_df = tmp_df.drop(['SUBWAY_NUM', 'BUS_CNT', 'BIKE_NUM'], axis=1)\n",
    "\n",
    "    # 교육\n",
    "    tmp_df['교육'] = (0.07) * tmp_df['MID_SCH_NUM'] + (0.03) * tmp_df['HIGH_SCH_NUM'] + tmp_df['ACADEMY_NUM'] * (0.7) + (\n",
    "        0.9) * tmp_df['ELE_SCH_NUM']\n",
    "    tmp_df = tmp_df.drop(['MID_SCH_NUM', 'HIGH_SCH_NUM', 'ACADEMY_NUM', 'ELE_SCH_NUM'], axis=1)\n",
    "\n",
    "    # 육아\n",
    "    tmp_df['육아'] = tmp_df['CHILD_MED_NUM'] + tmp_df['KINDER_NUM']\n",
    "    tmp_df = tmp_df.drop(['CHILD_MED_NUM', 'KINDER_NUM'], axis=1)\n",
    "\n",
    "    # 치안\n",
    "    tmp_df['치안'] = tmp_df['POLICE_NUM'] + tmp_df['CCTV_NUM'] + tmp_df['FIRE_NUM']\n",
    "    tmp_df = tmp_df.drop(['POLICE_NUM', 'CCTV_NUM', 'FIRE_NUM'], axis=1)\n",
    "\n",
    "    # 건강\n",
    "    tmp_df['건강'] = (0.94) * tmp_df['HOSPITAL_NUM'] + tmp_df['PHARM_NUM']\n",
    "    tmp_df = tmp_df.drop(['HOSPITAL_NUM', 'PHARM_NUM'], axis=1)\n",
    "\n",
    "    # 편의시설\n",
    "    tmp_df['편의시설'] = 0.04 * tmp_df['DPTM_NUM'] + 0.44 * tmp_df['CON_NUM'] + 0.25 * tmp_df['CAFE_NUM'] + 0.27 * tmp_df[\n",
    "        'RETAIL_NUM']\n",
    "    tmp_df = tmp_df.drop(['DPTM_NUM', 'CON_NUM', 'CAFE_NUM', 'RETAIL_NUM'], axis=1)\n",
    "\n",
    "    tmp_df.set_index('DONG_CODE', inplace=True)\n",
    "\n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def robust_scaling(df):\n",
    "    robust_scaler = RobustScaler()\n",
    "\n",
    "    robust_scaler.fit(df)\n",
    "\n",
    "    robust_data = robust_scaler.transform(df)\n",
    "    ro_df = pd.DataFrame(robust_data)\n",
    "    ro_df.index = df.index\n",
    "    ro_df.columns = df.columns\n",
    "    return ro_df\n",
    "\n",
    "\n",
    "def preprocessing_df():\n",
    "    area_df = merge_area_data()\n",
    "    assem_df = assembling_features(area_df)\n",
    "\n",
    "    tmp_data = assem_df.iloc[:, 3:]\n",
    "    df = tmp_data.div(assem_df['AREA'], axis=0)\n",
    "\n",
    "    max_lim_log_list = [\"교통\", \"치안\", \"교육\", \"COLIVING_NUM\", \"STARBUCKS_NUM\", \"MC_NUM\", \"NOISE_VIBRATION_NUM\", \"VEGAN_CNT\"]\n",
    "\n",
    "    for f in max_lim_log_list:\n",
    "        quan = df[f].quantile(0.95)\n",
    "        df[f] = np.where(df[f] > quan, quan, df[f])\n",
    "        df[f] = np.log1p(df[f])\n",
    "\n",
    "    max_lim_list = [\"LEISURE_NUM\", \"GOLF_NUM\", \"건강\", \"편의시설\"]\n",
    "    for f in max_lim_list:\n",
    "        quan = df[f].quantile(0.95)\n",
    "        df[f] = np.where(df[f] > quan, quan, df[f])\n",
    "\n",
    "    ro_df = robust_scaling(df)\n",
    "    ro_df = ro_df[['교통', '치안', '건강', '편의시설', '교육',\n",
    "             '육아', 'MZ_POP_CNT', 'COLIVING_NUM', 'VEGAN_CNT', 'KIDS_NUM',\n",
    "             'PARK_NUM', 'STARBUCKS_NUM', 'MC_NUM', 'NOISE_VIBRATION_NUM',\n",
    "             'SAFE_DLVR_NUM', 'LEISURE_NUM', 'GYM_NUM', 'GOLF_NUM', 'CAR_SHR_NUM',\n",
    "             'ANI_HSPT_NUM']]\n",
    "\n",
    "    return ro_df\n",
    "\n",
    "def first_clustering(df):\n",
    "    global tmp_df\n",
    "    basic_pca = PCA(n_components=2, random_state=0)\n",
    "    basic_pca_transformed = basic_pca.fit_transform(df)\n",
    "\n",
    "    # density_data = minmax_norm(density_data)\n",
    "    first_kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, random_state=0)\n",
    "    first_kmeans.fit(basic_pca_transformed)\n",
    "\n",
    "    basic_df = tmp_df.copy()\n",
    "    basic_df['km_cluster'] = first_kmeans.labels_\n",
    "\n",
    "    basic_df['pca_x'] = basic_pca_transformed[:, 0]\n",
    "    basic_df['pca_y'] = basic_pca_transformed[:, 1]\n",
    "\n",
    "    return basic_df, first_kmeans, basic_pca\n",
    "\n",
    "def second_clustering(basic_df, df,  user_first):\n",
    "    cluster_num = [3,3,2,0]\n",
    "    second_cluster = basic_df[basic_df['km_cluster'] == user_first]\n",
    "    cluster_data = df.loc[second_cluster.index.values]\n",
    "    second_pca = PCA(n_components=2)\n",
    "    second_pca_transformed = second_pca.fit_transform(cluster_data)\n",
    "    second_kmeans = KMeans(n_clusters=cluster_num[user_first], init='k-means++', max_iter=400, random_state=0)\n",
    "    second_kmeans.fit(second_pca_transformed)\n",
    "\n",
    "    cluster_tmp = second_cluster.copy()\n",
    "    cluster_tmp['km_cluster'] = second_kmeans.labels_\n",
    "    return second_kmeans, second_pca, cluster_tmp\n",
    "\n",
    "def create_category(df):\n",
    "    first_category = []\n",
    "    for column in df.columns[:6]:\n",
    "        category = []\n",
    "        for i in range(0,81,20):\n",
    "            x = (df[column].quantile(i/100) + df[column].quantile((i+20)/100)) / 2\n",
    "            category.append(x)\n",
    "        first_category.append(category)\n",
    "\n",
    "    second_category = []\n",
    "    for column in df.columns[6:]:\n",
    "        cate = [df[column].quantile(0.25), df[column].quantile(0.75)]\n",
    "        second_category.append(cate)\n",
    "\n",
    "    return first_category, second_category\n",
    "\n",
    "def user_scaling(first_category, second_category, user , df):\n",
    "    user_data = [0] * len(user)\n",
    "    select = [0] * len(user)  # 유저의 카테고리 선택여부 저장\n",
    "\n",
    "    for i in range(len(user[:6])):  # 첫번째 카테고리에 구간별 중앙값 부여\n",
    "        if (user[i] != 0):\n",
    "            user_data[i] = first_category[i][user[i] - 1]\n",
    "            select[i] = 1\n",
    "    for j in range(len(user[6:])):  # 두번째 카테고리에 평균을 중앙값으로 부여\n",
    "        if (user[j + 6] != 0):\n",
    "            user_data[j + 6] = second_category[j][1]\n",
    "            select[j + 6] = 1\n",
    "        else:\n",
    "            user_data[j + 6] = second_category[j][0]\n",
    "    user_df = pd.DataFrame(user_data, index=df.columns, columns=['user']).T\n",
    "    return user_df,select\n",
    "\n",
    "\n",
    "def weighting(user_df, df, select, user_name):\n",
    "    weight_df = pd.read_excel('1107_가중치.xlsx')\n",
    "    weight_df.rename(columns={'Unnamed: 0': '분류'}, inplace=True)\n",
    "    weight_df.fillna(0, inplace=True)\n",
    "    weight_df.set_index('분류', inplace=True)\n",
    "\n",
    "    values = user_df.loc[user_name].values\n",
    "    weight = weight_df[weight_df.columns].values\n",
    "    w = [1] * len(weight)\n",
    "    for i in range(len(weight)):\n",
    "        if(select[i] == 1):\n",
    "            for k in range(len(weight[i])):\n",
    "                w[i] += weight[i][k]\n",
    "\n",
    "    weighted_user_data = []\n",
    "    for i in range(len(values)):\n",
    "        weighted_data = values[i] * w[i]\n",
    "        weighted_user_data.append(weighted_data)\n",
    "    weighted_user_df = pd.DataFrame(weighted_user_data,index=df.columns,columns=['user']).T\n",
    "    return weighted_user_df\n",
    "\n",
    "\n",
    "# 유저 스케일 데이터 입력 시 해당 클러스터 출력 함수\n",
    "def user_clustering(basic_df, df, user_scaled, first_pca, first_kmeans):\n",
    "    user_pca = first_pca.transform(user_scaled)\n",
    "    user_first = first_kmeans.predict(user_pca)[0]\n",
    "\n",
    "    second_kmeans, second_pca, second_cluster = second_clustering(basic_df, df, user_first)\n",
    "    user_pca_2 = second_pca.transform(user_scaled)\n",
    "    user_second = second_kmeans.predict(user_pca_2)[0]\n",
    "    result_cluster = second_cluster[second_cluster['km_cluster'] == user_second]\n",
    "    return user_second, result_cluster\n",
    "\n",
    "def similarity(user_df, df, user_name, num): # 유저 데이터, 유사도 측정을 위한 데이터, 유저 이름, 원하는 순위\n",
    "    con_data = pd.concat([user_df.loc[[user_name]],df])\n",
    "    rc_sim = cosine_similarity(con_data,con_data)\n",
    "    sim_matrix = pd.DataFrame(rc_sim,columns=con_data.index).loc[[0]].T\n",
    "    rank = sim_matrix[0].sort_values(ascending=False) # 유사도 순서로 정렬\n",
    "    ranking = rank[1:num+1].index.tolist() # 1~n 위 리스트\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b54a4",
   "metadata": {},
   "source": [
    "'교통', '치안', '건강', '편의시설', '교육', '육아', 'MZ_POP_CNT', 'COLIVING_NUM',\n",
    "       'VEGAN_CNT', 'KIDS_NUM', 'PARK_NUM', 'STARBUCKS_NUM', 'MC_NUM',\n",
    "       'NOISE_VIBRATION_NUM', 'SAFE_DLVR_NUM', 'LEISURE_NUM', 'GYM_NUM',\n",
    "       'GOLF_NUM', 'CAR_SHR_NUM', 'ANI_HSPT_NUM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f95843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = [5,3,3,5,2,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9736beee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['교통', '치안', '건강', '편의시설', '교육', '육아', 'MZ_POP_CNT', 'COLIVING_NUM',\n",
       "       'VEGAN_CNT', 'KIDS_NUM', 'PARK_NUM', 'STARBUCKS_NUM', 'MC_NUM',\n",
       "       'NOISE_VIBRATION_NUM', 'SAFE_DLVR_NUM', 'LEISURE_NUM', 'GYM_NUM',\n",
       "       'GOLF_NUM', 'CAR_SHR_NUM', 'ANI_HSPT_NUM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0324441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yorijori\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing_df()\n",
    "\n",
    "basic_df, first_kmeans, first_pca = first_clustering(df)\n",
    "first_category, second_category = create_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc5777dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yorijori\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yorijori\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가산동' '삼성1동' '소공동']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yorijori\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_df,select = user_scaling(first_category, second_category, user,df)\n",
    "weighted_user_df = weighting(user_df, df, select, 'user')\n",
    "user_scaled = [weighted_user_df.loc['user'].values]\n",
    "user_group, user_include_df = user_clustering(basic_df, df , user_scaled, first_pca, first_kmeans)\n",
    "result_dong_list = similarity(user_df, df.loc[user_include_df.index.values], \"user\",3)\n",
    "print(user_include_df.loc[result_dong_list]['DONG'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f01f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb5882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954de38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
